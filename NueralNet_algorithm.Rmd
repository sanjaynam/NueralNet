
NueralNet algorithm from scratch

``{r setup, include=FALSE}
r = getOption("repos")
r["CRAN"] = "http://cran.r-project.org"
options(repos = r)
options(repos="https://cran.rstudio.com" )

```


```{r}

NeuralNetwork <- R6Class("Neural Network", public = list(
  X=NULL, Y=NULL, W1=NULL, W2 = NULL, 
  output=NULL,

initialize = function (formula, hidden, data = list()){
  #Model and training data
  mod <- model.frame(formula, data=data)
  self$X <- model.matrix(attr(mod,'terms'), data = mod)  # Input matrix -I/P layer
  self$Y <- model.response(mod) # Iris type
  
  #Dimentions
  D <- ncol(self$X) # - I/p dimentions - check Ip matric size (iris = 5)
  K <-length(unique(self$Y))  # - O/p classes
  H <- hidden #code here - hidden layers = 5 for iris
    
  #Initial weight and bias
  self$W1  <- matrix(rnorm(D*H),nrow=D,ncol = H)  #initializing martix with normal distribution.
  self$W2   <- matrix(rnorm((H+1)*K),nrow=H+1,ncol = K) 
  },

  #Activation Functions
  sigmoid = function(z) {1/(1+exp(-z))},
  dsigmoid =function(z) { z*(1-z)},   
  softmax = function(z){exp(z)/rowSums(exp(z))}, 

  #Feedforward Nueral Network - Fit function
  fit = function(data=self$X){
    h <-self$sigmoid(data %*% self$W1)  # a1 = sigma(input layer * weight 1)
    score <- cbind(1,h) %*% self$W2 
    return(self$softmax(score))         # Output layer
  },
  
  feedforward = function(data=self$X){
  self$output <- self$fit(data)
  invisible(self)
  },
  
  #Backpropagate function
  backpropagate = function(lr = 1e-2) {
    h <- self$sigmoid(self$X %*% self$W1)
    Yid <- match(self$Y, sort(unique(self$Y))) # Extracting label values.
    
    haty <- self$output - (col(self$output) == Yid) # (yhat - y)
    dW2 <- t(cbind(1,h))%*% haty
    
    dh <- haty %*% t(self$W2[-1, ,drop = FALSE])
    dW1 <- t(self$X) %*% (self$dsigmoid(h) * dh)
    
    # Calculating new weights = old weight - learning rate*Derivative rate
    self$W1 <- self$W1 - lr * dW1  
    self$W2 <- self$W2 - lr * dW2
    
    invisible(self)
  },
    #Predict function
      perdict = function(data=self$X){
      probs <- self$fit(data)
      preds <- apply(probs,1, which.max) # Locate largest value
      levels(self$Y)[preds]
    },
    
    #Compute loss
    compute_loss = function(probs = self$output){
      Yid <- match(self$Y, sort(unique(self$Y)))
      correct_logprobs <- -log(probs[cbind(seq_along(Yid),Yid)])
      sum(correct_logprobs)
    },
  
    #Train function
    train = function(iteration = 10000,
                     learn_rate = .0001,
                     tolerance = .01,
                     trace = 100) {
      for (i in seq_len(iteration)) {
        self$feedforward()$backpropagate(learn_rate)
        if (trace > 0 && i %% trace ==0 ) 
          message ('Iteration ', i, '\tLoss ', self$compute_loss(), 
                  '\tAccuracy ' ,self$accuracy())
      if (self$compute_loss() < tolerance) break
      }
      invisible(self)
     },
      
    accuracy = function() {
      predictions <- apply(self$output, 1, which.max)
      predictions <- levels(self$Y)[predictions]
      mean(predictions == self$Y)
    }
))


```

Apply iris dataset with 5 hidden layers in the above algorithm to predict species using all predictors

```{r}
iris_cpy <- read.csv("Iris.csv")
summary(iris_cpy)

irisnet <- NeuralNetwork$new(Species ~ SepalLengthCm + SepalWidthCm + PetalLengthCm +     PetalWidthCm, data=iris_cpy, hidden = 5)
irisnet$train(10000, trace = 100, learn_rate = .0001)

```
